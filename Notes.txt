These will be general notes i take for ai stuff especially useful functions n shit to use during the events.

Pandas:
.read_csv -> self explanatory
.info() -> gives general info about the set
.head() -> to have the first rows of the data
.describe() -> gives more detailled info as a summary about the columns for example mean and median and std etc.
.corr() -> gives the correlation matrix between every feature where the closer it is to 1 the more correlated the value are (closer to 1 values grow up the same closer to -1 values go different ways the same)

from pandas.plotting import scatter_matrix(dataframe, *other usual plot args*) -> this is a shit ton of graphs that talk about correlation (recommended to use for features that have promising correlation with the target)

MatPlotLib:
adding the graph name on a pandas dataframe usually works
.hist -> main parameter is bins(number of bars) and fig_size(general in almost every graph for the size)

ScikitLearn (sklearn):
from sklearn.model_selection import train_test_split(dataframe, test_size=0.2 (ratio of test), (addtitional strat you could add:stratify=housing["income_cat"]), random_state=42 (seed))
from sklearn.Imputer import SimpleImputer -> generally used for imputing Na values in your dataframe, parameters: strategy["mean","most_frequent",("constant", (additional parameter) fill_value=...)] NOTE: the imputer returns a numpy array on transform NOT a dataframe
from sklearn.preprocessing import OrdinalEncoder -> just gives text categories numbers
from sklearn.preprocessing import OneHotEncoder -> separates features into 0 and 1 table called dummies just look it up cuh (also you can use pd.get_dummies which does the same thing)
from sklearn.preprocessing import MinMaxScaler(feature_range(-1,1)) -> scaler to get values between the range
from sklearn.preprocessing import StandardScaler -> different calculation that uses the standard deviation to not get anomalies to squish the values in minmax scaler
from sklearn.metrics.pairwise import rbf_kernel -> THIS FUNCTION NEEDS A LOT OF TALKING but in a nutshell it gives you a correlation between the distance of a value and the feature. useful in numerical values to know what numbers to pick for clustering later.
.inverse_transform() -> availlable in some models that allow for the label or target to be "unscaled" after scaling them for whatever reason it may be (distribution was winged for the target)
from` sklearn.preprocessing import FunctionTransformer -> useful for creating new custom transformers for your data. advanced topic for now as in Chapter 2. Study it later by yourself through scikit learn docs duh.
from sklearn.pipeline import Pipeline -> used to do everything at once and takes as input a tuple of a string name and a transformer
from sklearn.pipeline import make_pipeline -> if you are lazy to name just use this and add your transformers
from sklearn.compose import ColumnTransformer -> used to separate columns for specific steps of the pipeline steps. useful for separating tasks and takes as input a tuple(name, pipeline, feature names)
from sklearn.compose import make_column_selector, make_column_transformer -> too lazy to name column transformer, and the selector is also for the lazy people who dont want to declare the labels of the features of each pipe, make column selector has as an argument the type needed.
from sklearn.multiclass import OneVsRestClassifier -> forces a binary alg to use OvR strat takes as input the model or pipe
from sklearn.multiclass import OneVsOneClassifier -> forces a binary alg to use OvO strat takes as input the model or pipe
from sklearn.multioutput import ClassifierChain -> estimator for the chain classification strat for the multioutput strat, look at the docs ig.

from sklearn.metrics import root_mean_squared_error -> RMSE used for calculating performance of a regression model (could also user mean_squared_error) input (predictions, target)
from sklearn.metrics import accuracy_score -> acc score takes test then predicted
from sklearn.metrics import confusion_matrix -> lots of yap about this will dedicate an entire section for it. takes the Y_test and Y_predicted as output
from sklearn.metrics import ConfusionMatrixDisplay ->  fancy way to plot the confusion matrix same input as 
from sklearn.metrics import precision_score, recall_score, f1_score -> gets the scores related to the confusion matrix. and takes as input (test, predictions)
from sklearn.metrics import precision_recall_curve -> just prepares data to draw the precision recall curve as a plot and see their relation. takes (test, predictions) and returns 3 values: precisions, recalls, thresholds
from sklearn.metrics import roc_curve -> receiver operating characteristic, similar to precision recall curve but insted gives out fpr (false positive rate) and tpr (true positive rate), takes as input (test, predictions) and returns fpr, tpr, thresholds
from sklearn.model_selection import cross_val_score -> just look it up bruh input (model, train, test, scroring="(look for docs)", cv=splits)
from sklearn.metrics import roc_auc_score -> calculates the surface under and takes as input (test, predictions)
from sklearn.model_selection import cross_val_predict -> same as cross_val_score but instead returns the predictions too for all values (doesnt take a scoring arg)
from sklearn.model_selection import GridSearchCV -> will do cross evalutaion while trying out all the possible combinations of hyperparameters you feed it input (model/pipeline, param_grid, cv=splits, scoring="you decide")
from sklearn.model_selection import RandomGridSearchCV -> same shit but can be faster with gamba technique (also HalvingRandomGridSearchCv is a thing too)
from sklearn.model_selection import ShuffleSplit -> shuffles the data for n number and returns arrays of the training and test indexes.
from sklearn.metrics import make_scorer -> can be very useful when working with a model that requires a unique score or cost function like pAUC


NOTES IN GENERAL:
in supervised learning especially with regression the process of making an efficent model is:
look at the data offered and pick the best data to use.
split your shit indeed. (NOTE: any transformation applied on the train data should be applied to the test data)
preprocess the data by finding patterns and correlations to know which one to pick.
try to get the stuff into bell shapes or some shit idk.
classified data turns into dummies if there are few.
impute any null values if few else just get rid of them entirely.
scale the values to be around 0-1 or -1 1 depending on the model your using.
test multiple models on the train set and validation set to see if any perform good and dont overfit(perform too well on the train data and not very well on the validation data use more complex model) or underfit(just shit numbers use less complex number or get more features) also use cross evaluation dummy
find the best hyperparameters with GridSearchCV or any of the techniques
to evaluate the model you may either use RMSE or MSE (RMSE can guess the overall performance without focus on the outliers much compared to the MSE)

in supervised learning this time for classification:
to rate a model's evaluation we can either rely on accuracy (predicted/test) but that generally isnt the best way to describe how well a model does especially in skewed datasets
the confusion mastrix is used in binary classification (either one thing or the other) and shows 4 informations
[TN,FP] 
[FN,TP]
TN -> true negatives is the number of 0s the model guessed correctly
TP -> true positives is the number of 1s the model guessed correctly
FP -> false positives is the number of 0s the model gussed as 1
FN -> false negatives is the number of 1s the model gussed as 0
from this matrix we can deduce the following information for better information about its performance
TP/(TP+FP) is called precision: the ratio of which the model correctly got the number's 1. this metric is mostly useful if you want your model to give the least amount of wrong guesses at the price of less values gussed (model is more strict)
TP/(TP+FN) is called recall or true positive rate: the ratio of which the model got the most amount of 1s. this metric is mostly useful if you want your model to give the most amount of 1s regardless of how many 0s guessed. (model just targets anything that looks a bit like the 1)
(preicision*recall)/(precision + recall) is called the F1 score (harmonic mean), this score is more impartial and a better metric to use to rate how well your model made guesses as it gives more weight to the low values. this metric is mostly useful if you want the best balance of precision and recall in your classifiers. 
FP/(FP+TN) is called false positive rate FPR or 1 - specificity useful or also fall out to get the ratio of correct guesses the model made among all the 1s.
in the receiver operating characteristic ROC curve which is a curve simmilar to the precision recall curve. it takes this time instead recall or TPR and FPR as a plot and draws a curve which shows the models overall performance for which the closer it is to the corner the better the perforamnce of the model.
another way to get more precise information about the performance is to calculate the area beneat the roc curve called auc or area under curve where the closer it is to 1 the better it is. 

multiclass classification: when you want the predictions to be more than 2 values (binary classification) in this case you may rely on 2 strategies, OvR (one versus rest) or OvA (one versus all) which is a strategy which includes training multiple binary models for each class and mark the other one as the "rest" and then the predictions will be done by making those models compete and the model with the most confidence will decide the prediction
another strategy is OvO (one versus) meaning you put every binary classifier in duels and the winner of the tournament (literally) decided the prediction, it sounds dumb but it works.
multilabel classification: when the output should include multiple outputs that are binary, for example in face recognition you want an output that has every face it can recognize on the image and output those multiple faces instead of 1 output. a strat for this is chaining classifiers from one to another meaning a classifier guesses the first label and is followed by another one later to guess with the data + 1st classifier prediction
multioutput classification: when the output is multilabel and multiclass meaning a mix of multiclass and multioutput (multilabel) classification. 

Chapter 4:

From Chapter 4 and going on every talk will be about models their hyperparameters and their general use cases

> Linear Regression:

from sklearn.linear_model import LinearRegression

y = Θ*x
Θ: parameters
x: feature values

self explanatory, just draws a line that averages the features.

the model is trained through the normal equation that finds the lowest parameter value Θ for the cost function which could be the MSE or RMSE

> Gradient Decent:

Technique that through the cost function uses steps to converge to the local minimum of a cost function, starting with a random value Θ 
Gradient Decent hyperparameters are as follows:
max_iter -> the maximum iterations or steps the model should stop at
tol -> tolerance or when the model should stop if the improvement is less than the tol
eta0 -> learning rate or how wide the steps the model takes
loss -> cost function or loss function the gradient uses to train on 
penalty -> regularization techniques (refer to Ridge and Lasso)

to compute the gradient decent's next step you need the derivative of the loss function

the formula looks something like this Θ(next step) = Θ - learning_rate(or eta0)*MSE'(Θ)

to implement them with sklearn you will need to 
import sklearn.linear_model import SGDClassifier or SGDRegressor

> Polynomial Regression:

in case your data follows a certain polynomial pattern + Noise you have the ability to use polynomial regression to train linear models on such features
through importing 
from sklearn.preprocessing import PolynomialFeatures
the parameters of the PolynomialFeattures class is degree for the polynomial degree you believe the data should follow and include_bias and includ_interactions for the interaction between the features (x1*x2 and x1*x3 and so on) and their bias(where the degree is 0 as in x1^0)
training a linear model on a new dataset that went through polynomialfeatures makes it possible to get predictions that are polynomial and not linear

> Regularized LinearRegression models:

in most cases it is always better to use regularized models to dodge the case of overfitting your model. this is done by adding a regularization term to the cost function to limit the model from shooting for parameters way too high for what they need to be
for this 2 models are availlable:

Ridge:
from sklearn.linear_model import Ridge
with the hyper parameter of alpha (between 0 and 1) controlling how regularized the model should be. as alpha goes higher the parameters end up being more grounded and closer to 0

J(Θ) = MSE(Θ) + (alpha/m)*sum(Θ^2) (J being the cost function and sum being the total of every parameter squared)

this regluarization term is also called the l2 norm

Lasso:
from sklearn.linear_model import Lasso
almost similar to the Ridge function with the new regularization term being l1 norm instead of l2:
J(Θ) = MSE(Θ) + 2*alpha*sum(Θ)
Lasso model tends to eliminate weights of the least important features so it automatically performs feature selection

ElasticNet:
from sklearn.linear_model import ElasticNet
middle ground between Ridge and Lasso with a parameter l1_ratio that determines who takes more importance between the l2 norm and l1 norm:
J(Θ) = MSE(Θ) + (r)2*alpha*sum(Θ) + (1-r)(alpha/m)*sum(Θ^2)

to know which model to pick it is generally better to use ridge but at any suspiscion of having useless features it is better to use ElasticNet or Lasso

> Logistic Regression:

Model that gives the probabilty of the value instead of the normal output of a linear model through a sigmoind function giving out an S shape instead of a line and the output becoming a probability

p = Sig(Θ.T * X) with Sig(t) = (1+exp(-t))^-1
the result p is then put through the step function h where
y = h(x) = {x >= 0.5 -> 1 | x < 0.5 -> 0}

the cost function is the negative log of the probabilites dependant of the target y = 1 or 0
or in a better way written as:
J(Θ) = -(1/m)*sum(y*log(p) + (1-y)*log(1-p))
to use the logistic regression model
from sklearn.linear_model import LinearRegression
the hyperparameters:
penalty -> the regularization term used
C -> regularization strength inverse higher means less regularized

the softmax regression is automatically used when the model is trained on multi class output 

NOTES: 
since early stopping is simply a concept ill leave it here in the notes section of the chapter, this technique is reliant on training the model partially and note the scores of the model through out the training process and in the case of 
seeing the score go down then up it is recommend to stop the trainnig at that point instead of overfitting the model, this can be done through using the function of the models in partial_fit() of the models which allows you to train the model over the already trained parameters


Chapter 5:

SVM or support vector machines are models that are able to do linearn and non linear classification and regression.

> Linear SVM Classification:

how classification works in SVM is through calculating a boundary(class separation) and streets(outliers separation or anything that shouldnt be of both classes) that separate the data into 2 classes.

from sklearn.svm import linearSVC or SVR for regressions
with the hyperparameter 
C -> determining the street size as C gets larger the streets gets narrower
probabilty -> if set to True returns the probabilites of the functions and allows the use of predict_proba()

you can also use the decision_function() metod of the svm classes to get the confidence score which is the distance of the instance from the boundary
SVM's can be useful in the case of separating data that is not linear through the use of polynomial regression to help the model train on non linear datasets

kernels are very also depending on the classification task and the dataset shape. RBF kernel can help for non linearly separable solutions and Poly kernel each of them have their own hyperparameters

Chapter 6:

> Decision Trees

decision trees are in the simplest terms an if else nest hell. the tree will take a random feature and divide the features into boundaries then compare the features in its nodes until reaching the leaves which decided what the output will be.
each node of the tree has a gini impurity value which describes the difference of training instances that belong to either child of leaves. with gini=0 meaning the node is pure and every instance belongs to the class
the gini impurity value could be witched to Entropy which is a measure of uncertainity similar to gini impurity where the lower the entropy is the purer the node is. there isnt a huge difference between both an gini is faster to compute so its recommended to stick with gini.

form sklearn.tree import DecisionTreeRegressor | DecisionTreeClassifier
the hyperparameters:
max_depth = the layers the tree can have as a max number
max_features = max features evaluated in each split
max_leaf_nodes = max leaf number
min_samples_split = required samples needed in the node to split
min_samples_leaf = required number for a leaf to be created
min_weight_fraction_leaf = same as min_samples_leaf but as a fraction instead of the total number
criterion = (for regression) the criteria or the cost funtion of each node that needs to be minimized

increasing min parameters and decreasing max parameters can regularize the model.
Regression is calculated in the decision tree through splitting the data with value boundaries for each feature then calculating the minimal arg for the cost function which is usually mse

NOTE:
decision trees are sensitive to data position if the data was to be clustered it is recommended to rotate it in a way so that the tree could make the least amount of splits possible to classify the data

Chapter 7:

Ensembles are a collection of models that make multiple predictions and through a voting classifier can get the needed prediction. generally ensembles are better than a single model alone.
the classes that can be used to work with multiple models is a Voting Classifier in sklearn
imported through:
from sklearn.enseble import VotingClassifier (look at the docs idiot)

Traning multiple models at the same can be very costy so we refer to 2 sampling techniques called Bagging and Pasting which just take a random sample of values from the dataset.
the difference between bagging and pasting is that bagging can allow to have repeating samples and pasting does not allow that
these can be imported as
from sklearn.ensemble import BaggingClassifier
the hyper parameters:
estimator -> the model 
n_estimators -> number of models
max_samples -> number of samples to get from the training set
max_features -> number of features to pick from the training set
boottrap -> if samples are drawn with replacement (pasting)
oob_score -> if you want out of bag to be applied for evaluation 

because of this random sampling process the data is not fully used to train the ensemble most of the time there is a certain ratio of that data that is never trained which is about 37%
these 37% of data can be used to evaluate the model and this technique is called OOB or out of bag evaluation. to get the score just call oob_score_ after fitting the model

> Random Forests:
from sklearn.ensemble import RandomForestClassifier | RandomForestRegressor
has all the hyperparameters of a decisiontree with the additional parameter of n_estimators -> number of models

> ExtraTrees:
from sklearn.ensemble import ExtraTreesClassifier | ExtraTreesRegressor
similar to the random forests however the way the data is split here is more random which allows for more variance and less bias, the hyperparameters are the same

Forests are very useful to know how important some of the features are through calling the value of feature_importances_ after training

> Boosting:
a technique where you take weak learners and train them to become one independant strong learner meaning from multiple eh performing models to an ensemble of a robust one
> AdaBoosting:
a technique where data is fit to a weak learner then the result turns into weights then feeds the new weights into a new model and so on
from sklearn.ensemble import AdaBoostClassifier | AdaBoostRegressor
hyperparameters:
estimator -> the model
n_estimator -> number
learning_rate -> you just know

> GradientBoosting:
same as ADA but feeds the residual errors instead of the new weights to the next models.
from sklearn.ensemble import GradientBoostingClassifier
this one doesnt have an estimator as it automatically uses trees.
loss -> the loss functions
learning_rate
n_estimators
has about the same parameters as the forest classifier
for very large datasets use HistGradientBoosting which allows to separate the data into bins and train separately

> Stacking:
the technique of feeding the output of multiple model's outputs into another model type shit.
from sklearn.ensemble import StackingClassifier
which takes as parameters a dict of estimators an a final_estimator

Chapter 8:
Dimensionality reduction is used to simplify the data and help with data visualization this can mean the projection of n features into a m number of features with m < n and some info loss.
the main technique used for this is to imagine it as projecting the data into a lower dimension plane for example a 3d plane into 2d and a 2d plane into a line etc. 
OR manifold learning which is trying to represent a higher dimensional shape into a lower one while trying to keep its shape for example a swiss roll data set to be unfolded in 2d.
we want the data to be reduced in a way the variance is kept at its highest.
> PCA:
calculates the vectors at which the variance of data is highest and lowest and projects it into the lower plane through these new vectors.
from sklearn.decomposition import PCA
parameters:
n_components -> number of features or dimensions to leave
svd_solver -> method used to calculate the PCA
we can calculate the information loss through 1 - the total of explained_variance_ratio_ accessible in the class after fitting

>Random Projection:
it is said that when we have very high dimensionality randomly removing one of the dimensions and projecting them on a lower plane will not lose as much data as you might think.
from sklearn.random_projection import johnson_lindenstrauss_min_dim
n_samples = the number of instances there are
eps = the data you wanna lose
and with 
from sklearn.random_projection import GaussianRandomProjection
n_components -> dimensions you want
eps -> data loss

>LLE:
doesnt rely on projections but looks for linearly similar features and treis to find a low dimensional representation of it. very useful whent there are twisted patterns in the dataset like rolls.
from sklearn.manifold import LocallyLinearEmbedding
n_neightbors -> number of values that are close to the feature to consider to reduce
n_components -> the number of features you want

Chapter 9:
unsupervised learning is used for many tasks that are related most of the time to finding patterns in the dataset you use. mainly clustering anomaly detection and density estimation.
to cluster your data you can use 
> KMeans
from sklearn.cluster import KMeans (can import MiniBatchKmeans or look for a better algorithm for accelerated learning n shi)
n_clusters -> number of clusters
max_iter -> iterations the model will stop at if no improvement is seen
tol -> tolerance at which if the closters move less than it will stop the model 
n_init -> if set to 1 can include the cluster's centriod positions
SCORING IN UNSUPERVISED:
generally there isnt a good way to score in unsurpervised learning.
for scoring we can either refer to inertia found in a clustering model's values wich tells you how spread apart the instances are compared to the clusters
to find the best number of clusters if we dont know what it is we can either use dimensionality reduction to try and visualize the date or calculate the inertia the lower it is the better but usually not the best metric
silhouette_score() imported through sklearn_metrics which gives out the mean of how well a data fits the clusters and a better alternative to calculate the best cluster number for kmeans clusters
the best way to pick a cluster  number however is through a silhouette diagram and picking the most balanced values in terms of score and cluster density
k means is usually good for round same sized blobs of data to detect however it doesnt perform well in weirdly shaped data forms like moons.

k means can be good for semi supervised tasks as it can get the most relevant instances of a data in clusters and labelling them and training a model on them could allow you to easily bump up the accuracy when having very low number of instances

> DBSCAN:
a better clustering algorithm used to find shapes that are not blobs however this idiot cant fit and predict for shit.
from sklearn.cluster import DBSCAN
eps -> minimum distance for an instance to be a part of a cluster
min_samples -> number of samples required for an instance to be called a core instance meaning for other instances to look at it as a cluster neighbor

> Gaussian Mixture
this model assumers that the dataset was generated through gaussian distributions mixed together and tries to cluster the data accoding to that. very useful if the data has shapes of waves or elipses.
from sklearn.mixture import GaussianMixture
n_components -> number of mixtures
covariance_type -> shape of the gaussian mixtures can be used for regularization
this alg can be used for anomaly detection where the further an instance is from a certain threshold it gets classified as an anomaly
to select the number of clusted we can rely on the AIC and BIC criterion. they result in similar models most of the time, BIC tends to give simpler models than AIC but doesnt fit large data as well AIC
both can be found in the class by calling .aic(data) or .bic(data) the lower they are the better
you could alternatively use Bayesian Gaussian Mixture Models
from sklearn.mixture import BayesianGaussianMixture
which automatically tries to find the best number of clusters needed for the data

### PART 2: NEURAL NETWORKS:

Chapter 10:
composition of Artificial Neural Networks.. with an input layer, hidden layers and an output layer.
each layer is composed of neurons that calculate a specific function called activation functions. these neurons get activated when a certain number of inputs goes into them.
the simplest form of a neuron is called a perceptron that returns a value between 0 and 1
each neuron will take as input a vector of values, and has as parameters a vector of weights for each vector + 1 bias parameter
the perceptron adds to the calculations of this a heap function or step function that convert the end result into 0 or 1 where if the result is > 0 ut returns 1 else 0
you can mess around with the Perceptron class in sklearn.Linear_model.Perceptron
the layers of an ANN are represented as an input layer with is the train data generally 
multiple hidden layers in the middle that feed its output to another layer (when all the results are input into another layer its called a deep neural network)
an output layer which could be your prediction of a confidence score.

the way ANNs are trained is with backpropagation which uses GD to calculate the step and tweak the parameters of each neuron by taking a batch of samples
passing first through the layers and keeping track of the weights that contributed the most to the output and tuning their weights.
the neurons parameters need be randomized when starting so that they are not correlated and result to the same neuron in every layer.

an MLP is a multi layer perceptron neural network which can be used for classification and regression tasks
can be imported in sci-kit learn through
from sklearn.neural_networks import MLPRegressor | MLPClassifier

but because thats lame we grow up and use Kerras in Tensorflow 
to create models in Tensorflow we can use 3 ways:
Sequential:
tf.keras.Sequential()
where you can add layers with tf.keras.layers (input layers, Flatten, Dense layers,) and giving out their Neuron number and activation function
by calling add to the sequential model or through a list inside Sequential([layers])
you can return the details of the model by calling summary() on model
you can return the layers of the model by calling layers
you can return the weights and biases by calling get_weights()

lastly after generating a model you will need to compile it by calling compile() and setting the loss function, optimizer and metrics you wanna check on while running the model
lastly you will be able to train the model by calling fit(train,test, epochs, validation_data=(X_valid,y_valid)) and can give out a validation data for the metrics
calling fit returns the history which is the data of each epoch that can be used to evaluate the performance of your model through out the training time.
you can find the parameters through history.params, epochs through history.epochs, and a dictionary for evyrthing through history.history that can be used to visualize the stats on pandas
to test predictions you would call the predict() function that returns the predictions

Functional:
where you declare every layer as a variable alone for example
Input_layer = tf.keras.layers.{LAYER NAME}
hidden_layer = tf.keras.layers.Dense(n_neurons)
output_layer = tf.keras.layers.Dense(n_neurons)

then getting the output of each layer by feeding it the input layer of another:
input_ = Input_Layer[X_train]
hidden = hidden_layer[input_]
output = output_layer[hidden]

(_ is written after input to not overwrite the default Input function of python)
and finally you can create your model by calling Model on keras and setting the input and output layers
model = tf.keras.Model(inputs=[input_], outputs=[output])
and compiling it
model.compile(loss, optimizer, metrics) 
and train and test just like the Sequential model

Functional models allow you to customize the input and output and leaves you more freedom to mess around with the layers of the ANN which is why its better than sequential in complex architecturial models
for example in the case of wanting to get multiple output layers since your model had 3 output neurons you would give out a list of 3 outputs when building the model

and itha nta m9wed you can write the model as a subclass of tf.keras.Model with
class MyModel(tf.keras.Model):
    def __init__() where you can set the layers of the model
    def call() where you can return any result you want like a raito between the result of a metric on train and valid and others

the Model class needs to have a call() method for it to work but generally it is recommended to use sequential or functional methods as its more flexible to work around with compared to hard coding the layers in a class

NOTE THAT PREPROCESSING LAYERS LIKE NORMALIZERS ETC NEED TO BE ADAPTED TO THE DATA BEFORE TRAINING AND PREDICTING

to save a model you can call the model.save() function of the model which will save the entire model and to load it you just call keras.models.load_model(model_name) to return the model and its parameters for each neuron
else you could also call save_weights() which is more lightweight and saves the connections weights biases prerocessing optimizers etc which will be very useful for callbacks

Callbacks:
in the fit function of a model you could add a callbacks parameters which allows you to save edit or extract information while training the model for example adding a checkpoint for the model to track
its progress in case a crash happens or using earlystoppind to stop the training in case there is no improvement or the model starts to overfit
you can find all the callback methods that are built in in keras.callbacks or you could write your own callback function with keras.callbacks.Callback

when creating a callback class you have access to on_train_begin on_train_end functions that when the model starts to fit or predict or evauluate the functions will get executed for each epoch

Lastly you could use TensorBoard which is a webui that reads the checkpoints left in the directories and is useful to visually and track the models progress and get info from the model
since im lazy i wont explain how its done and will rely on god to find out how to set it up in any projecting

Neural Networks Hyperparameters fine tuning:
you can either use scikerras to wrap your model into a scikitlearn model and use grid and randomized search on it or use 
keras_tuner named as kt
kt is a function that does random search by calling kt.RandomSearch(model_builder, max_trials, overwrite, directory, project_name) where you can feed it a model_builder function that creates your model
given random instances of how many layers neurons and activation funcitions and optimizers and your tuner will randomly select these parameters and return the result of each neural network
you can extract the best model with get_best_models(num_models) and find the parameters with get_best_hyperparameters()

each tuner is guided by an oracle which tells the tuner what the next random search parameters should be and keeps track of the trials of the random search
returned by calling get_best_trials() and calling summary() to find a summary of the trial

in case you have data preprocessing you might use a different method that model_builder function which is creating a sub class of kt.HyperModel which needs a build function that works just like the model_build function
and a fit function which can have hyperparameter tuning for preprocessing data and returns the fit method for the main model

some tips for selecting hyperparameters:
the number of layers depends on the task at hand but a good way to pick a number is to start with 1 hidden layer and increase the size until your model overfits
the number of neurons per layer should generally be less than the inputs and could either converge to a smaller number or every layer could contain the same number of neurons
learning rate just start small and gradually build up (note changing any other hyperparameter will change how the learning rate is affected so remember to change it when modifying any other parameter)
batch-size could be a number between 2 and 32 or the maximum your gpu ram could take in in both cases the performance doesnt change a lot
activation function ReLU is generally good for the hidden layers as for the ouput it depends on the task at hand as for the output if its multi output classification use softmax if its binary classification use sigmoind
Number of iterations only used for early stopping and doesnt really need to be tweaked

Chapter 11:
in this chapter we will meet with the great antagonist vanishing/exploding gradient problems where the calculation of derivatives for gradient decent may either get almost to a 0 value or explode to a very high value which may result in divergence
this will result in weird weight training being weird (very slow for vanishing and too extreme and chaotic for exploding) this has many causes one of them is the activation function and the other is due to weight initialization
to fix this we present the following methods to optimize the way neurons learn:

Glorot and He Initialization:
Glorot: assuming the sigmoid or tanh funciton is used as active functions or other linear activations
where the weights will be initialized in a Nomral or uniform distribution where the variance is 1/fan_avg or r is the sqrt(3/fan_avg) for normal distribution
with fan_avg being fan_in / fan_out called the xavier initialization (fan_in stands for the number of inputs and fan_out stands for the number of output)
replacing fan_avg with fan_in will give you LeCun initialization for the nerds who wanna know how it works  
He: assuming the ReLU or any of its families is used as acive functions 
the weights will be initialized in a Normale or uniform distribution where the variance is 2/fan_in or r is the sqrt(6_fan_in) for the normal distribution
to use these Initialization techniques you would need to only add kernel_initializer="initializer name" to your layers
you could also check out VarianceScaling for more freedom on the weight initialization 

Better Activation Functions:
ReLU is kinda ass since its shape can cause some neurons to die (weight is just 0) this happens when ReLU inputs is negative for all
instead of this we can use variations of ReLU:
LeakyReLU: leaves a small leak with the parameter a that determines the scale of the "leakage" on the negative, where it takes max(az,z)
Random LeakyReLU: alpha is randomized for each neuron and pickes as a mean in testing
Parametric Leaky ReLU (PReLU): same as LeakyReLU only difference is a is a parameter that can be trained in backpropagation instead of being set by the user
to use these functions you could find LeakyReLU and PReLU as layers in keras.layers (use He Initialiazation) Random ReLU is not in keras yet

these functions suffer from one main thing which is that they are not smooth and abruptly change at z=0 for that we have alternatives like the following:

ELU: exponential Linear Unit where when z<0 ELU=a(exp(z)-1) which gives a smooth curve on the negative. ELU has many amazing properties like taking on negatize z values,
nonzero gradient which removes the issue of dead neurons and for a=1 the function is smooth everywhere
scaled ELU (SELU): same as ELU with some scaling (a=1.67 so about 1.05 ELU) this function helps self normalize its outputs with some conditions:
standarized inputs, initialization must be with LeCun_normal, works with MLPs, without regularization. under these conditions SELU would outperform any other neural network
to use them just call the activaion parameter on the layers "elu" or "selu"

GELU: another smooth variant of ReLU with the formula z*CDF(z). goated function for complex tasks as gradient decent might find it easier to fit complex patterns
Swish: even more goated than GELU with the formula z*std(beta*z) with beta a hyperparameter, the scaling of Beta helps even more with gradient decent 
Mish: z*tanh(softplus(z)), same goat as GELU and Swish and outperformed the others, takes the best of GELU and Swish in a way
keras supports activation="gelu" and "swish" but not mish yet.

Batch Normailzation:
technique to normalize the inputs of each layer for stable traning.
im not writing the function for that it just calculates the mean and std and normalizes each training batch. however it may show issues when the batches are very small like under 8 or smth idk
to use it in keras just add a kers.Layers.BatchNormalization before each Dense layer input and it will do its own magic.
for deep networks it can be very op to use so dont forget it. also has some paramets like momentum for like the change rate of the inputs and axis for which axis to normalize or some shit idk.

Gradient Clipping:
limit the gradients of back propagation to reduce the risk of gradient explosion. can be set by clipvalue or clipnorm when creating an optimizer like SGD

or if youre smart you could Reuse Pretrained Layers that is called transfer learning
you would find a DNN and take its first layers and freeze the weights then add new layers on top depending on the task and retrain the new model to realize the task you want
this technique is useful generally when you cant find enough data or label enough of it to train a neural network for it from scratch so you would find a neural network that has
similar goals to yours and freeze all the layers apart from the last of the layers you stole and add new layers to it for example the output you want
you can select layers of a model in keras with Model.layers and pick the firest layers then add on top new layers with .add(layers)
make sure to clone the model with clone_model() and load its weights because cloning only takes its architecture.
to lock layer weights you can set the layer.trainable False before training for some epochs then continue with a lower learning rate for the next epochs with all of the layers unfrozen

Unsupervised Pretraining:
an old technique that relies on training each layer individually and lock its weights and add new layers on top,
when no data is availlable you could pretrain some layers on unlabeled data on autoencoders or GANs, then take the lower layers of GAN's and auto_encoders to fit your supervised training goals by adding output layers
this was inspired by an old technique where layers would be trained on unlabeled data then weights frozen and add new layers and train them an freeze and so on.
it was called the greedy-layer-wise Pretraining

pretraining on Auxiliary task:
if its fucked and you cant find any data for your task then train a model on a task that kinda has the same goals like trying to detect hair styles you would train a model for face recognition then steal the lower layers to train a new model for your niche task

Optimizers:
SGD base class is ass which why we use lots of different optimizers than SGD alone
Momentum: keeps track of past gradient decents to build momentum which turns the decent into an acceleration and helps to speed up training against plateaus and may jump local minimas
implementation is by setting SGD(momentum=n) 
Nesterov Accelertated Gradient:same philosophy of Momentum however adds a bit of the last gradient decent to the cost function derivative so that it overshoots slightly the gradient which helps a lot for reaching the global minima actually
implementation is through setting momentum and nesterov=True
AdaGrad:takes the square of past gradients into a vector and does some magic to calculate the next step, goal of the alg is to decay the learning rate to be faster in steep dimensions. called an adaptive learning rate when such a thing is done
implementation is by using the AdaGrad optimizer
RMSProp: better AdaGrad as it solves one issue adagrad had which is it may slow down a bit too fast and never converges to the global optimum, this alg has an absurdly long function but it just adds a decay function to slow down the slowing down process, understand that however you want
implementation is with the RMSProp optimizer and habing rho and learning_rate as parameters with rho deciding how much it decays, typically 0.9
Adam: the goat, mix of RMSProp and momentum and im not gonna talk a lot about it, its just good.
implementation works with Adam optimizer and setting beta_1 (momentum decay or whatever) and beta_2 (decay of RMSProp or rho)
AdaMax: replaces the l2 norm to l_inf norm and makes it more stable but depends on the dataset so youll just have to experiment.
implemented with AdaMax optimizer as always
Nadam: Adam with Nestetov trick on acceleration so might converge slightly faster than Adam
implements just like any other optimizer
AdamW: W Chat, adds weight decay to the Adam optimizer which multiplies the weights by a factor of usually 0.99 to keep the weights small and have the model better generalize
implementation is hidden in optimizers.experimental.AdamW

Learning Rate Scheduling:
some tchniques for faster technique include scheduling the learning rate as a function or something

Power Scheduling: learning rate is a functiong of iteration number t where N(t)= N0/(1+t/s)*c. c is power and s are steps and are hyperparameters after s steps the rate dropw to N0/2 and then N0/3 and so on
Exponential Scheduling: N(t) = N0*0.1*(t/s) gradually drops by a facto of 10 every steps
Piecewise constant scheduling: slows down for a number epochs like N0=0.1 for 5 epochs then N1=0.001 for 50 epochs and so on
Performance Scheduling: validation error evy N steps and reduce the learning rate by the factor of error
1cycle Scheduling: linearly increase from N0 to N1 then from N1 to N0 halfway, N0 is usually set to 0.1*N1

to add power scheduling just set decay in SGD() optimizer (inverse of s)
exponetial can be set as a function then create a learning rate scheduler with callbasck.LearningRateScheduler
same for piecewisse
performance is set with ReduceLROnPlateau in callbacks

Avoiding Overfitting:
L1 and L2 Regularization: you know how it works already just add them as keras.regularizers.l2 or l1
Dropout:strat of dropping out p percentage of nodes every epoch and adjust the weights without the help of other neurons. done by adding Dropout layers before each Dense Layer
MCDropout:type shi that uses a weird trick for reducing the probabilities guessed by the model or smth idk. 
Max-Norm Regularization: caps the weights such as || w ||2 < r where r is the max_norm_parameter and ||.||2 is the l2 norm, reducing r increases the regularizatipn and doesnt allow the weights to go high
implementation is done with kernel_constraint=keras.constraints.max_norm(norm) when creating a layer

Chapter 12:
fuck you im not doing it its all sub classes and they all have the same __init__ call build and get_config functions go look at the docs like a grown ass man